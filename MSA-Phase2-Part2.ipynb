{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1154a7ca-ca00-48b5-a3dc-7047bf82c01a",
   "metadata": {},
   "source": [
    "## MSA 2024 Phase 2 - Part 2 Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45964d4a-9662-4a26-b363-5db9d266c90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!/opt/anaconda3/bin/python -m pip install joblib\n",
    "!/opt/anaconda3/bin/python -m pip install Cython \n",
    "!/opt/anaconda3/bin/python -m pip install pystan==2.19.1.1\n",
    "!/opt/anaconda3/bin/python -m pip install prophet\n",
    "!/opt/anaconda3/bin/python -m pip install xgboost\n",
    "!/opt/anaconda3/bin/python -m pip install keras \n",
    "!/opt/anaconda3/bin/python -m pip install tensorflow\n",
    "!/opt/anaconda3/bin/python -m pip install statsmodels\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input,LSTM, Dense\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from prophet import Prophet\n",
    "import tensorflow as tf\n",
    "import sklearn.metrics as skm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f7e8dd-0db9-4f7a-b7d7-925f0ba28949",
   "metadata": {},
   "source": [
    "### 1.Data normalization and definition of model evaluation indicators and model dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aac7e1d-88c1-45a0-9d8e-238cff138646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 归一化\n",
    "# 需要归一化的列\n",
    "columns_to_scale = ['Weekly_Sales', 'Temperature', 'Fuel_Price', 'CPI','Unemployment','Size']\n",
    "\n",
    "# 初始化一个字典来存储每个周的归一化器\n",
    "scalers = {}\n",
    "\n",
    "for week, df in data_frames.items():\n",
    "    # 使用副本进行操作，避免改变原始数据\n",
    "    df_scaled = df.copy()\n",
    "    \n",
    "    # 为每个周创建一个新的归一化器\n",
    "    scaler = MinMaxScaler()\n",
    "    df_scaled[columns_to_scale] = scaler.fit_transform(df_scaled[columns_to_scale])\n",
    "    \n",
    "    # 将归一化器存储在字典中\n",
    "    scalers[week] = scaler\n",
    "    \n",
    "    # 更新归一化后的数据回字典\n",
    "    data_frames[week] = df_scaled\n",
    "    \n",
    "    # 查看归一化数据\n",
    "    print(f\"Normalized data set for {week}:\\n{df_scaled.head()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320f3f85-1e0a-4a5b-b1a2-2c68a9a6090a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate the metrics including MSE\\MAE\\RMSE\\R-square\n",
    "def calculate_metrics(y_true,y_pred):\n",
    "    mse=skm.mean_squared_error(y_true,y_pred)\n",
    "    mae=skm.mean_absolute_error(y_true, y_pred)\n",
    "    rmse=np.sqrt(mse)\n",
    "    r2=skm.r2_score(y_true, y_pred)\n",
    "    non_zero_idx = y_true != 0\n",
    "    # Calculate MAPE \n",
    "    mape = np.mean(np.abs((y_true[non_zero_idx] - y_pred[non_zero_idx]) / y_true[non_zero_idx])) * 100\n",
    "\n",
    "    # Calculate SMAPE\n",
    "    smape = 100 * np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred)))\n",
    "\n",
    "    # Calculate MPE\n",
    "    mpe = np.mean((y_true[non_zero_idx] - y_pred[non_zero_idx]) / y_true[non_zero_idx]) * 100\n",
    "\n",
    "    return mse, mae, rmse, r2, mpe, mape, smape\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016d46bc-c27c-4ab1-bf2b-4f764dcba2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LSTM model\n",
    "def create_lstm_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        LSTM(50, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "# Creat neural network model\n",
    "def create_nn_model(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, activation='relu', input_shape=(input_dim,)))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06dce11-b8a5-48c7-b7b2-f29d7f3d0ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of features for LSTM\n",
    "# Add some other needed features \n",
    "additional_features = ['Store', 'Dept', 'Type', 'IsHoliday']\n",
    "\n",
    "num_features = len(columns_to_scale) + len(additional_features)\n",
    "lstm_model = create_lstm_model((1, num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc349654-174c-48bc-99e6-9309503e33d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models dictionary\n",
    "models = {\n",
    "    'LR': LinearRegression(),\n",
    "    'Ridge': Ridge(alpha=10.0),\n",
    "    'Lasso': Lasso(alpha=0.1),\n",
    "    'DT': DecisionTreeRegressor(max_depth=5, min_samples_split=50),\n",
    "    'RF': RandomForestRegressor(n_estimators=200, max_depth=10, min_samples_split=10),\n",
    "    'XGB': XGBRegressor(learning_rate=0.01, max_depth=10, n_estimators=500, subsample=0.8,colsample_bytree=0.8),\n",
    "    'LSTM': create_lstm_model,  \n",
    "    'NN': create_nn_model\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d673f50a-1150-420f-a338-470cca8d2f4b",
   "metadata": {},
   "source": [
    "### 2. Load and split preprocessed data\n",
    "### 3. Choose an algorithm\n",
    "### 4. Train and test a model\n",
    "### 5. Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba1a81f-a04f-4103-9fe3-b8a0fadc9e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump\n",
    "\n",
    "# 迭代每个DataFrame\n",
    "for week, df in data_frames.items():\n",
    "    # 获取当前周数，假设 week 格式为 'Weekly_Sales_Xw'\n",
    "    current_week = int(week.split('_')[2][:-1])\n",
    "    \n",
    "    # 选择float64和int64类型的列\n",
    "    numerical_df = df.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "    # 准备训练和测试数据\n",
    "    if f'Weekly_Sales_{current_week}w' in numerical_df.columns:\n",
    "        y = numerical_df[f'Weekly_Sales_{current_week}w']\n",
    "        # 删除所有不相关的销售周和预测列\n",
    "        columns_to_drop = [col for col in numerical_df.columns if 'Weekly_Sales_' in col  or 'Mark'  in col]\n",
    "        X  = numerical_df.drop(columns=columns_to_drop)\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n",
    "\n",
    "        print(f\"Processing model for week {current_week}\")\n",
    "\n",
    "        # 训练模型并进行预测\n",
    "        for model_name, model in models.items():\n",
    "            # 为当前模型准备数据\n",
    "            X_train_next, X_test_next = X_train.copy(), X_test.copy()\n",
    "            if model_name == 'LSTM':\n",
    "                # 重新初始化 LSTM 模型\n",
    "                lstm_model = model((1, X_train_next.shape[1]))\n",
    "                X_train_reshaped = X_train_next.values.reshape((-1, 1, X_train_next.shape[1]))\n",
    "                X_test_reshaped = X_test_next.values.reshape((-1, 1, X_test_next.shape[1]))\n",
    "                lstm_model.fit(X_train_reshaped, y_train, epochs=50, verbose=0)\n",
    "                y_train_pred = lstm_model.predict(X_train_reshaped).flatten()\n",
    "                y_test_pred = lstm_model.predict(X_test_reshaped).flatten()\n",
    "                model_to_save = lstm_model\n",
    "            elif model_name == 'NN':\n",
    "                # 重新初始化 NN 模型\n",
    "                nn_model = model(X_train_next.shape[1])\n",
    "                nn_model.fit(X_train_next, y_train, epochs=50, verbose=0)\n",
    "                y_train_pred = nn_model.predict(X_train_next).flatten()\n",
    "                y_test_pred = nn_model.predict(X_test_next).flatten()\n",
    "                model_to_save = nn_model\n",
    "            else:\n",
    "                model.fit(X_train_next, y_train)\n",
    "                y_train_pred = model.predict(X_train_next)\n",
    "                y_test_pred = model.predict(X_test_next)\n",
    "                model_to_save = model\n",
    "            # 组合模型名和周数，创建唯一的文件名\n",
    "            filename = f'model_{model_name}_{current_week}w.joblib'\n",
    "            # 保存模型到指定的文件\n",
    "            dump(model_to_save, filename)\n",
    "        \n",
    "            # 计算并打印评估指标\n",
    "            mse, mae, rmse, r2, mpe, smape, mape = calculate_metrics(y_train, y_train_pred)\n",
    "            print(f\"Week {current_week}, Model {model_name} - Training Metrics: MSE={mse:.2f}, MAE={mae:.2f}, RMSE={rmse:.2f}, R2={r2:.3f}, MPE={mpe:.3f}%, SMAPE={smape:.3f}%, MAPE={mape:.3f}%\")\n",
    "\n",
    "            mse, mae, rmse, r2, mpe,smape, mape = calculate_metrics(y_test, y_test_pred)\n",
    "            print(f\"Week {current_week}, Model {model_name} - Testing Metrics: MSE={mse:.2f}, MAE={mae:.2f}, RMSE={rmse:.2f}, R2={r2:.3f}, MPE={mpe:.3f}%, SMAPE={smape:.3f}%, MAPE={mape:.3f}%\")\n",
    "        # 更新 data_frames 字典\n",
    "        data_frames[week] = df\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a29277e-99d3-4e60-8a24-3c538d5b239b",
   "metadata": {},
   "source": [
    "### 6. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c367003-da93-4558-bd8b-cd9fab29e677",
   "metadata": {},
   "source": [
    "Data Selection and Preparation:\n",
    "The dataset was centered on the \"w\" store, where tables containing features, stores, and sales were merged to create a comprehensive dataset. Columns with high missing values, such as MarkDown1-5, were excluded to maintain data integrity. Data normalization was applied to standardize the range of variables, ensuring all features contributed equally to model performance. The dataset was split into training and testing sets, with a 70/30 split to balance model training and validation.\n",
    "\n",
    "Choice of Algorithms:\n",
    "A diverse set of models was selected, including Linear Regression, Ridge, Lasso, Decision Tree, Random Forest, XGBoost, LSTM, Neural Network. These models cover a range of techniques from linear and regularized regression to tree-based and time-series forecasting, as well as deep learning. The selection aimed to explore different data patterns and relationships within the dataset. XGBoost was chosen for its efficiency and effectiveness in handling diverse data types.\n",
    "\n",
    "Evaluation Metrics:\n",
    "The models were evaluated using metrics such as MSE, MAE, RMSE, R2, MPE, SMAPE, and MAPE. These metrics provided insights into model accuracy, error magnitude, and percentage errors, which are crucial for assessing forecast performance. SMAPE and MAPE were particularly relevant for financial forecasting, offering insights into the error proportions relative to actual values.\n",
    "\n",
    "Training and Parameter Tuning:\n",
    "During initial training, some models showed signs of overfitting, indicated by discrepancies between training and testing metrics. To mitigate this, rigorous parameter tuning was conducted, particularly for XGBoost and neural networks. Adjustments to parameters like learning rate, tree depth, and the number of estimators were made to balance bias and variance, leading to improved model performance.\n",
    "\n",
    "Model Saving and Selection:\n",
    "After training, each model was saved using joblib for future use. XGBoost emerged as the best-performing model, offering a strong balance between accuracy and computational efficiency. The final selection of XGBoost was based on its superior performance across multiple evaluation metrics.\n",
    "\n",
    "Conclusion:\n",
    "This structured approach ensured the application of appropriate machine learning algorithms, leading to robust predictive models for sales forecasting. The thorough evaluation and refinement process, guided by multiple metrics, resulted in the selection of XGBoost as the most effective model, capable of generating reliable sales forecasts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
